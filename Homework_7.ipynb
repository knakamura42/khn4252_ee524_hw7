{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 7: Stock Prediction\n",
    "\n",
    "## GE Stock Prediction\n",
    "Here we are going to predict GE stock prices. Example from https://towardsdatascience.com/predicting-stock-price-with-lstm-13af86a74944"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "# Graph Functions\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (10,10) # Make the figures a bit bigger\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
    "from keras.utils import np_utils\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and Inspect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(os.getcwd())\n",
    "df_ge = pd.read_csv('ge_stock.txt', engine='python')\n",
    "df_ge.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.figure()\n",
    "plt.plot(df_ge[\"Open\"])\n",
    "plt.plot(df_ge[\"High\"])\n",
    "plt.plot(df_ge[\"Low\"])\n",
    "plt.plot(df_ge[\"Close\"])\n",
    "plt.title('GE stock price history')\n",
    "plt.ylabel('Price (USD)')\n",
    "plt.xlabel('Days')\n",
    "plt.legend(['Open','High','Low','Close'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the Data\n",
    "We need to normalize the data in order to improve convergence time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_cols = [\"Open\",\"High\",\"Low\",\"Close\",\"Volume\"]\n",
    "#train_cols = [\"Open\",\"High\",\"Low\",\"Volume\"]\n",
    "df_train, df_test = train_test_split(df_ge, train_size=0.8, test_size=0.2, shuffle=False)\n",
    "print(\"Train and Test size\", len(df_train), len(df_test))\n",
    "# scale the feature MinMax, build array\n",
    "x = df_train.loc[:,train_cols].values\n",
    "min_max_scaler = MinMaxScaler()\n",
    "x_train = min_max_scaler.fit_transform(x)\n",
    "x_test = min_max_scaler.transform(df_test.loc[:,train_cols])\n",
    "\n",
    "# Train scalar only on Y\n",
    "stock_min_max = MinMaxScaler()\n",
    "stock_min_max.fit_transform(np.reshape(df_train.loc[:, \"Close\"].values, (-1, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshape the Data for Training\n",
    "Our current dataset is not in a form suitable for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "img = mpimg.imread('stock_training.png')\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_timeseries(mat, y_col_index, TIME_STEPS):\n",
    "    # y_col_index is the index of column that would act as output column\n",
    "    # total number of time-series samples would be len(mat) - TIME_STEPS\n",
    "    dim_0 = mat.shape[0] - TIME_STEPS\n",
    "    dim_1 = mat.shape[1]\n",
    "    x = np.zeros((dim_0, TIME_STEPS, dim_1))\n",
    "    y = np.zeros((dim_0,))\n",
    "    \n",
    "    for i in range(dim_0):\n",
    "        x[i] = mat[i:TIME_STEPS+i]\n",
    "        y[i] = mat[TIME_STEPS+i, y_col_index]\n",
    "    print(\"length of time-series i/o\",x.shape,y.shape)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def trim_dataset(mat, batch_size):\n",
    "    \"\"\"\n",
    "    trims dataset to a size that's divisible by BATCH_SIZE\n",
    "    \"\"\"\n",
    "    no_of_rows_drop = mat.shape[0]%batch_size\n",
    "    if(no_of_rows_drop > 0):\n",
    "        return mat[:-no_of_rows_drop]\n",
    "    else:\n",
    "        return mat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_STEPS = 3\n",
    "BATCH_SIZE = 32\n",
    "x_t, y_t = build_timeseries(x_train, 3, TIME_STEPS)\n",
    "x_t = trim_dataset(x_t, BATCH_SIZE)\n",
    "y_t = trim_dataset(y_t, BATCH_SIZE)\n",
    "x_temp, y_temp = build_timeseries(x_test, 3, TIME_STEPS)\n",
    "x_val, x_test_t = np.split(trim_dataset(x_temp, BATCH_SIZE),2)\n",
    "y_val, y_test_t = np.split(trim_dataset(y_temp, BATCH_SIZE),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Creation\n",
    "Time to create our Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import functions from Keras\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(LSTM(100, batch_input_shape=(BATCH_SIZE, TIME_STEPS, \n",
    "                                            x_t.shape[2]), \n",
    "                                            dropout=0.0, \n",
    "                                            recurrent_dropout=0.0))\n",
    "lstm_model.add(Dropout(0.5))\n",
    "lstm_model.add(Dense(20,activation='relu'))\n",
    "lstm_model.add(Dense(1,activation='linear'))\n",
    "lstm_model.compile(loss='mean_squared_error', \n",
    "                   optimizer='sgd',\n",
    "                   metrics=['mae'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Model\n",
    "The following code is to plot your created model. If you do not have certain outside libraries installed, the code may not run. This is Okay. You do not need this code to do the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "from IPython.display import SVG\n",
    "#from keras.utils import model_to_dot\n",
    "plot_model(lstm_model, show_shapes = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Plot Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "history = lstm_model.fit(x_t, y_t,\n",
    "          batch_size=32, epochs=50, verbose=1,\n",
    "          validation_data=(trim_dataset(x_val, BATCH_SIZE), \n",
    "                           trim_dataset(y_val, BATCH_SIZE)))\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.figure()\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = lstm_model.evaluate(x_t, y_t, verbose=1)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Validation Data against Actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = lstm_model.predict(trim_dataset(x_val, BATCH_SIZE))\n",
    "display(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(stock_min_max.inverse_transform(predicted))\n",
    "plt.plot(stock_min_max.inverse_transform(np.reshape(trim_dataset(y_val, BATCH_SIZE), (-1,1))))\n",
    "plt.legend(['Estimated', 'Actual'], loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Changing Hyperparameters\n",
    "Keras comes with pre-implemented loss functions and optimizers:\n",
    "\n",
    "- Loss Function => https://keras.io/losses/\n",
    "- Optimizer     => https://keras.io/optimizers/\n",
    "         \n",
    "Please pick two loss functions and two optimizers (for a total of four combinations) and compare their results using 10 epochs. Include a plot of the actual stock values and the predicted values for each combination and comment your observation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Fill in the table here\n",
    "\n",
    "| Optimizer | Loss | Observations|\n",
    "|-----------|------|-------------|\n",
    "| Optimizer 1| Loss 1| Observation 1|\n",
    "| Optimizer 1| Loss 2| Observation 2|\n",
    "| Optimizer 2| Loss 1| Observation 3|\n",
    "| Optimizer 2| Loss 2| Observation 4|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: Create Your Own Stock Prediction Algorithm\n",
    "Follow similar steps to the above code, but for your own algorithm. Choose any stock and collect the data yourself (yahoo stocks is a good site).  \n",
    "\n",
    "Create your own deep learning architecture (fully connected, CNN, or LSTM) to predict future stock prices for **Thirty days** and **1 Year into the future**. Note: the example above only predicted the next dayâ€™s stock value. You are not limited to the feature extraction, preprocessing, or validation process used in part 1. Be creative! \n",
    "\n",
    "Briefly describe why you chose the deep learning architecture you used, along with reasons for choosing the number of epochs, optimizer, activation functions, number of neurons in each layer, batch size, and the validation methodology.\n",
    "\n",
    "Please turn in a .txt or .csv file that contains the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
